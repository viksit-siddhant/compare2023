{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from wav2vec2 import RequestsDataset, EmotionDataset, AudioModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from bert import TranscriptData, Classifier\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "tqdm.pandas()\n",
    "train_df = pd.read_csv('HCC/lab/train.csv')\n",
    "develdf = pd.read_csv('HCC/lab/devel.csv')\n",
    "testdf = pd.read_csv('HCC/lab/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3084/3084 [00:06<00:00, 511.12it/s]\n",
      "100%|██████████| 3084/3084 [00:07<00:00, 439.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3503/3503 [00:04<00:00, 814.97it/s]\n",
      "100%|██████████| 3503/3503 [00:07<00:00, 455.39it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "le = RequestsDataset.get_le(train_df,target='request')\n",
    "devel = RequestsDataset(develdf, data_path='HCC/wav/',le=le,max_sec=30,test=True,target='request')\n",
    "test = RequestsDataset(testdf, data_path='HCC/wav/',max_sec=30, test=True,target='request')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-french were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>ckpt_path = <span style=\"color: #808000; text-decoration-color: #808000\">'req_ckpts/wav2vec2-com.ckpt'</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>model = AudioModel(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 3 devel_loader = torch.utils.data.DataLoader(devel, batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>, shuffle=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>test_loader = torch.utils.data.DataLoader(test, batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>, shuffle=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>trainer = pl.Trainer(                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'devel'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0mckpt_path = \u001b[33m'\u001b[0m\u001b[33mreq_ckpts/wav2vec2-com.ckpt\u001b[0m\u001b[33m'\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0mmodel = AudioModel(\u001b[94m2\u001b[0m)                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 3 devel_loader = torch.utils.data.DataLoader(devel, batch_size=\u001b[94m16\u001b[0m, shuffle=\u001b[94mFalse\u001b[0m)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0mtest_loader = torch.utils.data.DataLoader(test, batch_size=\u001b[94m16\u001b[0m, shuffle=\u001b[94mFalse\u001b[0m)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0mtrainer = pl.Trainer(                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'devel'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt_path = 'req_ckpts/wav2vec2-com.ckpt'\n",
    "model = AudioModel(2)\n",
    "devel_loader = torch.utils.data.DataLoader(devel, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=16, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator='gpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at req_ckpts/wav2vec2-com.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from the checkpoint at req_ckpts/wav2vec2-com.ckpt\n",
      "/home/siddhant2021565/miniconda3/envs/COMPARE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3a1fdbb22941e2acaddf9fa35a66c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = devel.labels\n",
    "\n",
    "preds = trainer.predict(model,devel_loader,ckpt_path=ckpt_path)\n",
    "y_hat = torch.cat(preds).detach().cpu().numpy()\n",
    "print(recall_score(y,y_hat.argmax(axis=-1),average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at req_ckpts/wav2vec2-com.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from the checkpoint at req_ckpts/wav2vec2-com.ckpt\n",
      "/home/siddhant2021565/miniconda3/envs/COMPARE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6faa57f07642cdb8a33573dde2f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(model,test_loader,ckpt_path=ckpt_path)\n",
    "test_preds = torch.cat(test_preds).detach().cpu().numpy()\n",
    "testdf['request'] = le.inverse_transform(test_preds.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.to_csv('HCC/lab/test1.csv',index=False)\n",
    "bert_model = Classifier.load_from_checkpoint('distilbert_com/distil-com.ckpt')\n",
    "isinstance(bert_model,pl.LightningModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6822/6822 [00:02<00:00, 3339.06it/s]\n",
      "100%|██████████| 6822/6822 [00:02<00:00, 3364.35it/s]\n",
      "100%|██████████| 6822/6822 [00:00<00:00, 117381.18it/s]\n",
      "100%|██████████| 6822/6822 [00:00<00:00, 98447.41it/s]\n",
      "100%|██████████| 3084/3084 [00:00<00:00, 3505.83it/s]\n",
      "100%|██████████| 3084/3084 [00:00<00:00, 3638.88it/s]\n",
      "100%|██████████| 3084/3084 [00:00<00:00, 14195.11it/s]\n",
      "100%|██████████| 3084/3084 [00:00<00:00, 99736.56it/s]\n",
      "100%|██████████| 3503/3503 [00:01<00:00, 3476.57it/s]\n",
      "100%|██████████| 3503/3503 [00:00<00:00, 3543.66it/s]\n",
      "100%|██████████| 3503/3503 [00:00<00:00, 115467.38it/s]\n",
      "100%|██████████| 3503/3503 [00:00<00:00, 100865.31it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "traindata = TranscriptData(train_df, transcript_path='HCC/transcripts',target='complaint')\n",
    "devdata = TranscriptData(develdf, transcript_path='HCC/transcripts',test=True,target='complaint',le=traindata.le)\n",
    "testdata = TranscriptData(testdf, transcript_path='HCC/transcripts',test=True,target='complaint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:41<00:00,  4.61it/s]\n"
     ]
    }
   ],
   "source": [
    "devel_loader = torch.utils.data.DataLoader(devdata, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=16, shuffle=False)\n",
    "\n",
    "bert_model = bert_model.to('cuda:0')\n",
    "preds = []\n",
    "for batch in tqdm(devel_loader):\n",
    "    x0, x1= batch\n",
    "    x0 = {k:v.to('cuda:0') for k,v in x0.items()}\n",
    "    x1 = {k:v.to('cuda:0') for k,v in x1.items()}\n",
    "    y_hat = bert_model(x0,x1)\n",
    "    preds.append(y_hat.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5823259054627241\n"
     ]
    }
   ],
   "source": [
    "labels = devdata.df['label'].values\n",
    "y_hat = np.concatenate(preds).argmax(axis=-1)\n",
    "print(recall_score(labels,y_hat,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:46<00:00,  4.74it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for batch in tqdm(test_loader):\n",
    "    x0, x1= batch\n",
    "    x0 = {k:v.to('cuda:0') for k,v in x0.items()}\n",
    "    x1 = {k:v.to('cuda:0') for k,v in x1.items()}\n",
    "    y_hat = bert_model(x0,x1)\n",
    "    preds.append(y_hat.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>complaint</th>\n",
       "      <th>request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0001.wav</td>\n",
       "      <td>no</td>\n",
       "      <td>presta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_0002.wav</td>\n",
       "      <td>yes</td>\n",
       "      <td>presta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_0003.wav</td>\n",
       "      <td>yes</td>\n",
       "      <td>presta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_0004.wav</td>\n",
       "      <td>yes</td>\n",
       "      <td>affil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_0005.wav</td>\n",
       "      <td>no</td>\n",
       "      <td>affil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename complaint request\n",
       "0  test_0001.wav        no  presta\n",
       "1  test_0002.wav       yes  presta\n",
       "2  test_0003.wav       yes  presta\n",
       "3  test_0004.wav       yes   affil\n",
       "4  test_0005.wav        no   affil"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = np.concatenate(preds).argmax(axis=-1)\n",
    "testdf['complaint'] = traindata.le.inverse_transform(y_hat)\n",
    "submission = testdf[['filename','complaint','request']]\n",
    "submission.to_csv('HCC.csv',index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9925/9925 [00:10<00:00, 978.84it/s] \n",
      "100%|██████████| 9925/9925 [00:03<00:00, 2681.51it/s]\n",
      "100%|██████████| 9925/9925 [00:00<00:00, 12611.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10143/10143 [00:10<00:00, 948.68it/s]\n",
      "100%|██████████| 10143/10143 [00:04<00:00, 2454.72it/s]\n",
      "100%|██████████| 10143/10143 [00:01<00:00, 6109.01it/s]\n",
      "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-french were not used when initializing Wav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "develdf = pd.read_csv('emotions/lab/devel.csv')\n",
    "testdf = pd.read_csv('emotions/lab/test.csv')\n",
    "dev_emotions = EmotionDataset(develdf, data_path='emotions/raw/wav/', test=True)\n",
    "test_emotions = EmotionDataset(testdf, data_path='emotions/raw/wav/', test=True)\n",
    "\n",
    "ckpt_path = 'ckpts/epoch=6-step=9261.ckpt'\n",
    "model = AudioModel(9,task='emotions')\n",
    "devel_loader = torch.utils.data.DataLoader(dev_emotions, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_emotions, batch_size=16, shuffle=False)\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator='gpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at ckpts/epoch=6-step=9261.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from the checkpoint at ckpts/epoch=6-step=9261.ckpt\n",
      "/home/siddhant2021565/miniconda3/envs/COMPARE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ba406d146c4991981d84ed3fdb4c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(model,test_loader,ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10143, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Boredom</th>\n",
       "      <th>Calmness</th>\n",
       "      <th>Concentration</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Excitement</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Tiredness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41671.wav</td>\n",
       "      <td>0.074724</td>\n",
       "      <td>0.084136</td>\n",
       "      <td>0.368613</td>\n",
       "      <td>0.036996</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.692624</td>\n",
       "      <td>0.986331</td>\n",
       "      <td>0.034592</td>\n",
       "      <td>0.016126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41672.wav</td>\n",
       "      <td>0.199625</td>\n",
       "      <td>0.165747</td>\n",
       "      <td>0.392098</td>\n",
       "      <td>0.123404</td>\n",
       "      <td>0.340784</td>\n",
       "      <td>0.504053</td>\n",
       "      <td>0.376780</td>\n",
       "      <td>0.904248</td>\n",
       "      <td>0.198320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41673.wav</td>\n",
       "      <td>0.168761</td>\n",
       "      <td>0.575263</td>\n",
       "      <td>0.895132</td>\n",
       "      <td>0.415742</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.088157</td>\n",
       "      <td>0.170981</td>\n",
       "      <td>0.503815</td>\n",
       "      <td>0.374095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41674.wav</td>\n",
       "      <td>0.647625</td>\n",
       "      <td>0.112663</td>\n",
       "      <td>0.463723</td>\n",
       "      <td>0.266283</td>\n",
       "      <td>0.754371</td>\n",
       "      <td>0.586532</td>\n",
       "      <td>0.639892</td>\n",
       "      <td>0.156683</td>\n",
       "      <td>0.098347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41675.wav</td>\n",
       "      <td>0.054191</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.213714</td>\n",
       "      <td>0.094280</td>\n",
       "      <td>0.247939</td>\n",
       "      <td>0.996375</td>\n",
       "      <td>0.810094</td>\n",
       "      <td>0.117321</td>\n",
       "      <td>0.091916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename     Anger   Boredom  Calmness  Concentration  Determination   \n",
       "0  41671.wav  0.074724  0.084136  0.368613       0.036996       0.005835  \\\n",
       "1  41672.wav  0.199625  0.165747  0.392098       0.123404       0.340784   \n",
       "2  41673.wav  0.168761  0.575263  0.895132       0.415742       0.458657   \n",
       "3  41674.wav  0.647625  0.112663  0.463723       0.266283       0.754371   \n",
       "4  41675.wav  0.054191  0.142857  0.213714       0.094280       0.247939   \n",
       "\n",
       "   Excitement  Interest   Sadness  Tiredness  \n",
       "0    0.692624  0.986331  0.034592   0.016126  \n",
       "1    0.504053  0.376780  0.904248   0.198320  \n",
       "2    0.088157  0.170981  0.503815   0.374095  \n",
       "3    0.586532  0.639892  0.156683   0.098347  \n",
       "4    0.996375  0.810094  0.117321   0.091916  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.sigmoid(torch.cat(preds)).detach().cpu().numpy()\n",
    "print(y_hat.shape)\n",
    "submission = testdf[['filename']].copy()\n",
    "labels = list(testdf.columns[1:])\n",
    "for i in range(len(labels)):\n",
    "    submission[labels[i]] = y_hat[:,i].reshape(-1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('emotions.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMPARE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
